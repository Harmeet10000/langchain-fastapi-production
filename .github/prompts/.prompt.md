Create a production-grade FastAPI application integrating LangChain, LangGraph, and LangSmith with Google's Gemini models. The solution should follow a structured approach with the following requirements:

Architecture and Setup:
- Implement a clean, modular folder structure with functional programming and not OOPs where possible
- Configure Docker and docker-compose for development and production environments
- Set up environment variables and configuration management
- Implement comprehensive logging and monitoring

Core Components:
1. LangChain Integration:
   - Configure Gemini models as the primary LLM provider
   - Implement conversation memory management
   - Set up document loaders and vector stores for RAG
   - Create structured output parsers using Pydantic models

2. LangGraph Implementation:
   - Design workflow graphs for complex reasoning chains
   - Implement state management and persistence
   - Create reusable graph components

3. LangSmith Integration:
   - Configure tracing and monitoring
   - Set up evaluation metrics
   - Implement feedback loops

4. FastAPI Framework:
   - Create RESTful endpoints following OpenAPI standards
   - Implement authentication and authorization
   - Set up request validation using Pydantic
   - Configure CORS and middleware

5. Features:
   - Document processing and RAG pipelines
   - Multi-modal content processing (MCP)
   - Conversation management with context retention
   - Structured output generation
   - Caching and performance optimization

Please provide the following to proceed:
1. LangSmith API key and project settings
2. Gemini API credentials
3. Preferred vector store configuration
4. Specific use cases or domain requirements

